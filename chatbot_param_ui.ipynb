{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b52d915-1ec7-45ba-8d85-a5a51c039d9a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8e72b2-c3aa-4fd7-8b19-446f0eb45541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings #may delete this for line below\n",
    "\n",
    "#new stuff\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e71ef6-d482-49bb-b441-7ddeb734efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment file explicitly\n",
    "from pathlib import Path\n",
    "dotenv_path = Path('/Users/kyli/Documents/GitHub/lab_gpt/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Access variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_maps_api_key = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2221848-2b00-4dd6-b0d1-320486e846b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if openai_api_key:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('ugh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "768a21fd-d919-4479-be9c-6d2ea8a1b641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Create a temporary database with your files\\ndef load_db(file, chain_type, k):\\n    #Load document - need to load multiple files\\n    loader = PyPDFLoader(file)\\n    #Split the documents into pages\\n    documents = loader.load()\\n    #Split text\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\\n    docs = text_splitter.split_documents(documents)\\n    #Define embedding - turn text chunks into vectors\\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\\n    #Create vector database from data\\n    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\\n    #Define retriever\\n    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\\'k\\': k})\\n    #Create chatbot chain. Memory is managed externally\\n    qa = ConversationalRetrievalChain.from_llm(\\n        llm = ChatOpenAI(model_name=llm_name, temperature=0),\\n        chain_type=chain_type,\\n        retriever=retriever,\\n        return_source_documents=True,\\n        return_generated_question=True,\\n    )\\n    return qa\\n    '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Create a temporary database with your files\n",
    "def load_db(file, chain_type, k):\n",
    "    #Load document - need to load multiple files\n",
    "    loader = PyPDFLoader(file)\n",
    "    #Split the documents into pages\n",
    "    documents = loader.load()\n",
    "    #Split text\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    #Define embedding - turn text chunks into vectors\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    #Create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    #Define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={'k': k})\n",
    "    #Create chatbot chain. Memory is managed externally\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm = ChatOpenAI(model_name=llm_name, temperature=0),\n",
    "        chain_type=chain_type,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7b571b-fdc6-4aaf-a823-5a0df457f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a temporary database with your files\n",
    "def load_db_new(file, chain_type, k):\n",
    "    #Load document - need to load multiple files\n",
    "    loader = PyPDFLoader(file)\n",
    "    #Split the documents into pages\n",
    "    documents = loader.load()\n",
    "    #Split text\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    #Define embedding - turn text chunks into vectors\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    #Create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    #Define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={'k': k})\n",
    "    \n",
    "    llm = ChatOpenAI()\n",
    "\n",
    "    #Contextualize question\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, just \"\n",
    "        \"reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    \n",
    "    # Answer question\n",
    "    qa_system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use \"\n",
    "        \"the following pieces of retrieved context to answer the \"\n",
    "        \"question. If you don't know the answer, just say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the answer \"\n",
    "        \"concise.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc5d1cfb-abb4-484e-b533-134e68d5c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT TIME YOU CLOSE OUT OF JUPYTER NOTEBOOK ADD DOCARRAY TO CONDA ENV\n",
    "#!pip install docarray\n",
    "#!pip install panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d0ff5d-5a35-4d53-bed3-dfceeac4b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class using panel and parem that creates an interactive chatbot\n",
    "#that interacts with the database\n",
    "\n",
    "import param\n",
    "#Imports panel library, used to create interactive interfaces (dashboards,\n",
    "#widgets, etc) GUI\n",
    "import panel as pn\n",
    "\n",
    "#the cbfs class inherits from param.Parameterized\n",
    "class cbfs(param.Parameterized):\n",
    "    #List to store conversation history\n",
    "    chat_history = param.List([])\n",
    "    #String for chatbot's latest response\n",
    "    answer = param.String(\"\")\n",
    "    #Latest question sent to document database\n",
    "    db_query  = param.String(\"\")\n",
    "    #List of documents/context retrieved from database\n",
    "    db_response = param.List([])\n",
    "\n",
    "    #Initialize the class\n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        #Empty list to store the chat's visual elements (?)\n",
    "        self.panels = []\n",
    "        #Initialize one file for demonstration\n",
    "        self.loaded_file = \"/Users/kyli/Desktop/lab_gpt_example_docs/01_pathophys_tbi.pdf\"\n",
    "        #Calls load_db() method to create a QA omdel\n",
    "        self.qa = load_db_new(self.loaded_file,\"stuff\", 4)\n",
    "\n",
    "    #Loads new file into database\n",
    "    def call_load_db_new(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            #Change the style of the button\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db_new(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        #Clear chat history\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    #Process queries\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        #Store question and answer in chat history\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        #Update database query with new question\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        #Update database with response with new source documents\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer']\n",
    "        #Add user and chatbot responses to new rows\n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, background='#F6F6F6'))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    #Decorator (?)\n",
    "    @param.depends('db_query ', )\n",
    "    #Shows latest db query, or message if no queries have been made\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    #Shows database lookup results as a list of retrieved docs\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history')\n",
    "    #Shows conversation history with each exchange in a new row\n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    #Clears chat history\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e375ae0c-2c90-4d46-bbaf-1ef3b6f2269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docarray in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (1.26.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (3.10.7)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (2.8.2)\n",
      "Requirement already satisfied: rich>=13.1.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (13.9.4)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (2.32.0.20241016)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from pydantic>=1.10.8->docarray) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from pydantic>=1.10.8->docarray) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from rich>=13.1.0->docarray) (2.15.1)\n",
      "Requirement already satisfied: urllib3>=2 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from types-requests>=2.28.11.6->docarray) (2.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d5ee5d-e0af-4fb6-bd59-8d7ce1a6adfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import docarray python package. Please install it with `pip install docarray`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages/langchain_community/vectorstores/docarray/base.py:23\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(da_version[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(da_version[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m31\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the DocArrayHnswSearch VectorStore the docarray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >=0.32.0 is expected, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upgrade, please run: `pip install -U docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: To use the DocArrayHnswSearch VectorStore the docarray version >=0.32.0 is expected, received: 0.16.5.To upgrade, please run: `pip install -U docarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cb \u001b[38;5;241m=\u001b[39m cbfs()\n\u001b[1;32m      3\u001b[0m file_input \u001b[38;5;241m=\u001b[39m pn\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mFileInput(accept\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m button_load \u001b[38;5;241m=\u001b[39m pn\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mButton(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad DB\u001b[39m\u001b[38;5;124m\"\u001b[39m, button_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mcbfs.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/kyli/Desktop/lab_gpt_example_docs/01_pathophys_tbi.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#Calls load_db() method to create a QA omdel\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa \u001b[38;5;241m=\u001b[39m load_db_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_file,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mload_db_new\u001b[0;34m(file, chain_type, k)\u001b[0m\n\u001b[1;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#Create vector database from data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m db \u001b[38;5;241m=\u001b[39m DocArrayInMemorySearch\u001b[38;5;241m.\u001b[39mfrom_documents(docs, embeddings)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Define retriever\u001b[39;00m\n\u001b[1;32m     15\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: k})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages/langchain_community/vectorstores/docarray/in_memory.py:67\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        DocArrayInMemorySearch Vector Store\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_params(embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     68\u001b[0m     store\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages/langchain_community/vectorstores/docarray/in_memory.py:40\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_params\u001b[0;34m(cls, embedding, metric, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_params\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize DocArrayInMemorySearch store.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        **kwargs: Other keyword arguments to be passed to the get_doc_cls method.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     _check_docarray_import()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryExactNNIndex\n\u001b[1;32m     43\u001b[0m     doc_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_doc_cls(space\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab_gpt_env/lib/python3.12/site-packages/langchain_community/vectorstores/docarray/base.py:29\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the DocArrayHnswSearch VectorStore the docarray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >=0.32.0 is expected, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upgrade, please run: `pip install -U docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import docarray python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import docarray python package. Please install it with `pip install docarray`."
     ]
    }
   ],
   "source": [
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db_new, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp)\n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "pn.extension()\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbee93-78cf-47f3-936a-a158bdc63f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab_gpt_env)",
   "language": "python",
   "name": "lab_gpt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
